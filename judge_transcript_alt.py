import os
import json
import argparse
from pathlib import Path
from typing import Dict, Any

from dotenv import load_dotenv
from datetime import datetime
import re
from openai import OpenAI


def load_transcript(run_dir: Path):
    path = run_dir / "transcript.jsonl"
    if not path.exists():
        raise FileNotFoundError(f"No transcript.jsonl in {run_dir}")
    records = []
    with path.open() as f:
        for line in f:
            records.append(json.loads(line))
    return records


ROOM_KEYWORDS = [
    "juniper",
    "foothill",
    "yerba buena",
    "salons",
    "grand assembly",
    "atrium",
    "expo",
    "nobhill",
    "soma",
    "ballroom",
]


def contains_event_facts(text: str) -> bool:
    t = (text or "").lower()
    if not t.strip():
        return False
    # Dates/months/days
    if any(m in t for m in ["june ", " jun ", "tuesday", "wednesday", "thursday"]):
        return True
    # Times like 11:15 AM or 13:15
    import re

    if re.search(r"\b(?:[01]?\d|2[0-3]):[0-5]\d(?:\s?[ap]m)?\b", t):
        return True
    # Session IDs (6 digits)
    if re.search(r"\b\d{6}\b", t):
        return True
    # Room/venue keywords
    if any(k in t for k in ROOM_KEYWORDS):
        return True
    return False


JUDGE_VERSION = "alt-judge-v3-semantic-heuristics"


def judge_one(client: OpenAI, user_text: str, golden: str, actual: str) -> Dict[str, bool]:
    if not golden:
        return {"instruction_following": True, "kb_grounding": True}

    system = (
        "You are an independent evaluator. Judge one turn.\n"
        "Return strict JSON: {\"instruction_following\": boolean, \"kb_grounding\": boolean}.\n\n"
        "instruction_following = True if the assistant directly answers the user's question OR clearly\n"
        "advances the requested task (acknowledges the request and asks for required missing info).\n"
        "False only if it neither answers nor advances. Ignore tool-calling mechanics and factual correctness;\n"
        "instruction_following is about task handling, not accuracy.\n"
        "Special cases that SHOULD be marked instruction_following=True (regardless of factual accuracy, which is\n"
        "handled by kb_grounding):\n"
        "- If GOLDEN_TEXT avoids exact totals (e.g., 'do not calculate counts'), a qualitative summary like 'multiple'\n"
        "  plus brief examples counts as answering.\n"
        "- Recommendation turns: proposing two session suggestions for the user's stated day/interest counts as answering,\n"
        "  even if suggested titles differ or contain factual mistakes.\n"
        "- Speaker-existence questions: listing that speaker's sessions counts as answering, even if extra sessions\n"
        "  are included or details contain mistakes.\n\n"
        "kb_grounding = True unless the assistant states an explicit contradiction or incorrect fact\n"
        "relative to GOLDEN_TEXT. Extra details/paraphrases are fine. If no factual claims are made, set True.\n\n"
        "Semantic normalization (meaning over exact string match): numbers (digits/words), dates/times (formats),\n"
        "room labels (punctuation/colon variants), and names (case/initials/diacritics) should be treated as\n"
        "equivalent when they refer to the same entity.\n\n"
        "Additional kb_grounding rules:\n"
        "- If the assistant refuses/deflects and states no event facts (no dates/times/rooms/counts/\n"
        "  speakers/sessions), set kb_grounding=True.\n"
        "- If all stated dates/times/rooms/speakers in ASSISTANT_ANSWER are consistent with GOLDEN_TEXT\n"
        "  under the semantic normalization above, set kb_grounding=True (unless contradicted elsewhere).\n"
        "- Do not penalize omissions: listing a correct subset of items (e.g., two of three sessions) without\n"
        "  contradictions still counts as kb_grounding=True.\n\n"
        "Task-params rule: if the assistant acknowledges a task and asks for required missing parameters\n"
        "for any of these workflows, set instruction_following=True: submit_dietary_request(name, dietary_preference);\n"
        "submit_session_suggestion(name, suggestion_text); vote_for_session(name, session_id);\n"
        "request_tech_support(name, issue_description); end_session(none).\n\n"
        "When uncertain, prefer True.\n"
    )
    user = (
        "USER_INPUT:\n" + user_text +
        "\n\nGOLDEN_TEXT:\n" + golden +
        "\n\nASSISTANT_ANSWER:\n" + actual
    )

    resp = client.chat.completions.create(
        model=os.getenv("ALT_JUDGE_MODEL", "gpt-5"),
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ],
        response_format={"type": "json_object"},
        max_completion_tokens=3072,
        extra_body={"reasoning_effort": "high"},
        seed=11,
    )
    text = resp.choices[0].message.content or "{}"
    try:
        data = json.loads(text)
        result = {
            "instruction_following": bool(data.get("instruction_following", False)),
            "kb_grounding": bool(data.get("kb_grounding", False)),
        }
        # Heuristic: 'how many' questions â€” accept qualitative answers when GOLDEN_TEXT avoids totals
        if not result["instruction_following"]:
            ut = (user_text or "").lower()
            at = (actual or "").lower()
            if "how many" in ut:
                qualitative_markers = [
                    "multiple",
                    "variety",
                    "various",
                    "several",
                    "range of",
                    "a range",
                    "mix of",
                    "includes",
                    "features",
                ]
                if any(m in at for m in qualitative_markers):
                    result["instruction_following"] = True
        # Heuristic: recommendation listing (two or more session-like entries) counts as answering
        if not result["instruction_following"]:
            txt = (actual or "").lower()
            count_titles = txt.count("session title:")
            import re
            count_times = len(re.findall(r"\b(?:[01]?\d|2[0-3]):[0-5]\d(?:\s?[ap]m)?\b", txt))
            count_enums = 0
            if re.search(r"\b1\.|\(1\)", txt) and re.search(r"\b2\.|\(2\)", txt):
                count_enums = 2
            if count_titles >= 2 or count_times >= 2 or count_enums >= 2:
                result["instruction_following"] = True
        # No-facts pre-check: if the assistant states no event facts, force kb_grounding=True
        if not contains_event_facts(actual):
            result["kb_grounding"] = True
        # Room-whitelist heuristic: if IF is False but a known room is mentioned, consider it KB-consistent
        if (not result["instruction_following"]) and any(k in (actual or "").lower() for k in ROOM_KEYWORDS):
            result["kb_grounding"] = True
        return result
    except Exception:
        # Fall back; also apply no-facts pre-check
        return {
            "instruction_following": False,
            "kb_grounding": True if not contains_event_facts(actual) else False,
        }


def main():
    parser = argparse.ArgumentParser(description="Independent judge for a run")
    parser.add_argument("run_dir", help="Path to runs/<timestamp> directory")
    parser.add_argument("--only-turns", default="", help="Comma-separated list of turn indexes to judge")
    parser.add_argument("--only-tool-turns", action="store_true", help="Judge only turns that expect a tool call")
    args = parser.parse_args()

    load_dotenv()
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    run_dir = Path(args.run_dir)
    records = load_transcript(run_dir)

    from turns import turns as expected_turns

    out_path = run_dir / "alt_judged.jsonl"
    passes = {"tool_use_correct": 0, "instruction_following": 0, "kb_grounding": 0}
    total = 0

    model_name = None
    # Build a filter set for turns if requested
    only_turns: set[int] | None = None
    if args.only_turns.strip():
        try:
            only_turns = {int(x.strip()) for x in args.only_turns.split(',') if x.strip()}
        except Exception:
            only_turns = None
    with out_path.open("w", encoding="utf-8") as outf:
        for rec in records:
            idx = rec.get("turn")
            if idx is None or idx >= len(expected_turns):
                continue
            if only_turns is not None and idx not in only_turns:
                continue
            if args.only_tool_turns and not expected_turns[idx].get("required_function_call"):
                continue
            total += 1
            golden = expected_turns[idx].get("golden_text", "") or ""
            user = rec.get("user_text", "")
            ans = rec.get("assistant_text", "")
            if model_name is None:
                model_name = rec.get("model_name")
            # tool-use correctness from transcript.tool_calls vs required_function_call
            expected_fc = expected_turns[idx].get("required_function_call")
            tuc = False
            tcs = rec.get("tool_calls") or []
            if not expected_fc:
                tuc = len(tcs) == 0
            else:
                exp_name = expected_fc.get("name")
                exp_args = expected_fc.get("args", {})
                def norm_str(s: str) -> str:
                    import re
                    t = (s or "").lower().strip()
                    # normalize common variants
                    t = t.replace("can't", "cannot").replace("cant", "cannot")
                    t = t.replace("can not", "cannot")
                    t = re.sub(r"[^a-z0-9\s]", "", t)
                    t = re.sub(r"\s+", " ", t)
                    return t
                def tokenize(s: str) -> set[str]:
                    t = norm_str(s)
                    toks = set(t.split())
                    stop = {
                        "the","a","an","on","in","at","to","of","for","with","and","or","i","about","please",
                        # domain stopwords for proposals
                        "session","talk","workshop","hallway","track","title","suggestion","submitted"
                    }
                    toks = {w for w in toks if w not in stop}
                    # Expand common fused forms
                    if "opentelemetry" in toks:
                        toks.update({"open","telemetry"})
                    return toks
                def arg_match_simple(k, v_exp, v_act):
                    # session_id should match exactly
                    if k == "session_id":
                        return v_exp == v_act
                    # For strings, only normalized strict equality in simple path; all other cases go to LLM
                    if isinstance(v_exp, str) and isinstance(v_act, str):
                        return norm_str(v_exp) == norm_str(v_act)
                    return v_exp == v_act
                def topic_tokens(txt: str) -> set[str]:
                    t = norm_str(txt)
                    toks = set(t.split())
                    stop = {
                        "the","a","an","on","in","at","to","of","for","with","and","or","i","about","please",
                        "session","talk","workshop","hallway","track","title","suggestion","submitted","proposal","propose"
                    }
                    toks = {w for w in toks if w not in stop}
                    if "opentelemetry" in toks:
                        toks.update({"open","telemetry"})
                    return toks

                def arg_match_llm(k: str, v_exp, v_act) -> bool:
                    # Only use LLM if both are strings and simple check failed
                    if not (isinstance(v_exp, str) and isinstance(v_act, str)):
                        return v_exp == v_act
                    system = (
                        "Decide if the PROVIDED_ARG conveys the SAME MEANING as the EXPECTED_ARG for the tool argument named ARG_NAME.\n"
                        "Evaluate semantic equivalence, not verbatim match. Be liberal for short strings.\n"
                        "Ignore: case, punctuation, articles (a, an, the), simple prepositions (in/on/at), contractions (can't=cannot), and word order.\n"
                        "Accept near-synonyms and minor wording changes.\n"
                        "Return strict JSON: {\"reasonable\": boolean}. Set reasonable=true if they express the same request/fact; set false only if they refer to different content or contradict.\n\n"
                        "Examples:\n"
                        "- TRUE: EXPECTED_ARG: \"Cannot access location maps on the mobile app.\" PROVIDED_ARG: \"Can't access the location maps in the mobile app\"\n"
                        "- TRUE: EXPECTED_ARG: \"A session about open telemetry tracing.\" PROVIDED_ARG: \"OpenTelemetry tracing session\"\n"
                        "- FALSE: EXPECTED_ARG: \"Cannot access location maps on the mobile app.\" PROVIDED_ARG: \"Need vegetarian lunch options\""
                    )
                    user_msg = (
                        f"ARG_NAME: {k}\n"
                        f"EXPECTED_ARG (reference): {v_exp}\n"
                        f"PROVIDED_ARG: {v_act}\n\n"
                        f"USER_INPUT:\n{user}\n\nASSISTANT_ANSWER:\n{ans}"
                    )
                    try:
                        resp = client.chat.completions.create(
                            model=os.getenv("ALT_JUDGE_MODEL", "gpt-5"),
                            messages=[
                                {"role": "system", "content": system},
                                {"role": "user", "content": user_msg},
                            ],
                            response_format={"type": "json_object"},
                            max_completion_tokens=256,
                            extra_body={"reasoning_effort": "high"},
                            seed=41,
                        )
                        txt = resp.choices[0].message.content or "{}"
                        data = json.loads(txt)
                        ok = bool(data.get("reasonable", False))
                        if not ok and k == "suggestion_text":
                            # Fallback: topic overlap between USER_INPUT and PROVIDED_ARG
                            tu = topic_tokens(user)
                            ta = topic_tokens(v_act)
                            if len(tu & ta) >= 2:
                                ok = True
                        return ok
                    except Exception:
                        return False
                def tc_matches(tc):
                    if tc.get("name") != exp_name:
                        return False
                    args = tc.get("args", {})
                    for k,v in exp_args.items():
                        va = args.get(k)
                        if k == "session_id":
                            if va != v:
                                return False
                            continue
                        # try simple; if fails and both strings, consult LLM
                        if arg_match_simple(k, v, va):
                            continue
                        if isinstance(v, str) and isinstance(va, str):
                            if arg_match_llm(k, v, va):
                                continue
                        return False
                    return True
                tuc = any(tc_matches(tc) for tc in tcs)

            judged = judge_one(client, user, golden, ans)
            scores = {"tool_use_correct": tuc, **judged}
            for k in passes:
                passes[k] += 1 if scores.get(k) else 0
            outf.write(json.dumps({**rec, "scores": scores}) + "\n")

    summary = {
        "model_name": model_name,
        "alt_passes": passes,
        "turns_scored": total,
        "judge_version": JUDGE_VERSION,
        "last_scored_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
    }
    (run_dir / "alt_summary.json").write_text(json.dumps(summary, indent=2), encoding="utf-8")
    print(json.dumps(summary, indent=2))

    # Build analysis.txt with succinct reasons per failed category
    def normalize_room(txt: str) -> set[str]:
        t = (txt or "").lower()
        rooms = set()
        for k in ROOM_KEYWORDS:
            if k in t:
                rooms.add(k)
        return rooms

    def extract_times(txt: str) -> set[str]:
        t = (txt or "").lower()
        return set(m.group(0) for m in re.finditer(r"\b(?:[01]?\d|2[0-3]):[0-5]\d(?:\s?[ap]m)?\b", t))

    failures = []
    # Reload judgments to avoid keeping them in memory
    alt = [json.loads(l) for l in out_path.open()]
    for rec in alt:
        s = rec.get("scores", {})
        turn = rec.get("turn")
        user = rec.get("user_text", "")
        ans = rec.get("assistant_text", "")
        golden = expected_turns[turn].get("golden_text", "") or ""

        reasons = []
        # Tool-use explanation
        expected_fc = expected_turns[turn].get("required_function_call")
        tcs = rec.get("tool_calls") or []
        if expected_fc and s.get("tool_use_correct") is False:
            if not expected_fc and tcs:
                reasons.append(
                    f"Tool-use: unexpected tool call(s): {[tc.get('name') for tc in tcs]}"
                )
            elif expected_fc and not tcs:
                reasons.append(
                    f"Tool-use: expected {expected_fc.get('name')} but no tool call was made"
                )
            elif expected_fc and tcs:
                reasons.append(
                    f"Tool-use: expected {expected_fc.get('name')} args {expected_fc.get('args')} but no matching call; got calls: {tcs}"
                )

        # IF explanation
        if s.get("instruction_following") is False:
            reasons.append("IF: did not directly answer or advance the task this turn")

        # KG explanation with simple mismatch heuristics
        if s.get("kb_grounding") is False:
            a_times = extract_times(ans)
            g_times = extract_times(golden)
            a_rooms = normalize_room(ans)
            g_rooms = normalize_room(golden)
            if a_times and (a_times - g_times):
                reasons.append(
                    f"KG: time mismatch assistant {sorted(a_times)} vs KB {sorted(g_times)}"
                )
            if a_rooms and (a_rooms - g_rooms):
                reasons.append(
                    f"KG: room mismatch assistant {sorted(a_rooms)} vs KB {sorted(g_rooms)}"
                )
            if not reasons or all(not r.startswith("KG:") for r in reasons):
                reasons.append("KG: contains facts not aligned with GOLDEN_TEXT (judge false)")

        if reasons:
            failures.append(
                {
                    "turn": turn,
                    "user": user,
                    "assistant": ans,
                    "golden": golden,
                    "reasons": reasons,
                }
            )

    lines = [f"Model: {model_name}", f"Turns scored: {total}", ""]
    for fitem in failures:
        lines.append(f"Turn {fitem['turn']}: {fitem['user']}")
        lines.append("Assistant: " + fitem["assistant"].replace("\n", " ")[:500])
        lines.append("Golden: " + fitem["golden"].replace("\n", " ")[:500])
        for r in fitem["reasons"]:
            lines.append("- " + r)
        lines.append("")

    (run_dir / "analysis.txt").write_text("\n".join(lines), encoding="utf-8")


if __name__ == "__main__":
    main()
